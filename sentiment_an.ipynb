{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf1a5AhGbT5J",
        "outputId": "d5d27cdf-8aed-4ddc-8229-d1013cb9143b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('the_most_correct.csv').drop('tokenized_text',axis=1)\n",
        "X = data['Text']\n",
        "y = data['Emotion'].map({\n",
        "    'sadness':0,\n",
        "    'anger':1,\n",
        "    'love':2,\n",
        "\n",
        "    'fear':3,\n",
        "    'happy':4,\n",
        "    'surprise':5\n",
        "})\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk0Fd397b18f",
        "outputId": "68a32e21-1323-4d6f-cc90-001f870ff768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                  i didnt feel humiliated\n",
              "1        i can go from feeling so hopeless to so damned...\n",
              "2         im grabbing a minute to post i feel greedy wrong\n",
              "3        i am ever feeling nostalgic about the fireplac...\n",
              "4                                     i am feeling grouchy\n",
              "                               ...                        \n",
              "21454                 melissa stared at her friend in dism\n",
              "21455    successive state elections have seen the gover...\n",
              "21456                 vincent was irritated but not dismay\n",
              "21457    kendall-hume turned back to face the dismayed ...\n",
              "21458                      i am dismayed , but not surpris\n",
              "Name: Text, Length: 21459, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym5EiTxRdE4z",
        "outputId": "233dd583-674d-4b89-9911-3439522b1a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0\n",
              "1        0\n",
              "2        1\n",
              "3        2\n",
              "4        1\n",
              "        ..\n",
              "21454    3\n",
              "21455    3\n",
              "21456    3\n",
              "21457    3\n",
              "21458    3\n",
              "Name: Emotion, Length: 21459, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# our models first layer size lol :)\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "X = X.apply(lambda x: remove_stopwords(x))\n",
        "\n",
        "vectorizer.fit(X)\n",
        "\n",
        "vocab_size = len(vectorizer.vocabulary_)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_0L9VjheQ0l",
        "outputId": "c5f96ce5-3769-435b-fd82-94e1b0ded24a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19085"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vectorizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        text_vectorized = torch.Tensor(self.vectorizer.transform([text]).toarray()[0])\n",
        "\n",
        "        return text_vectorized, label"
      ],
      "metadata": {
        "id": "Zyhc0tRUe6_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size = 0.2, random_state = 42)\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train, vectorizer)\n",
        "test_dataset = TextDataset(X_test, y_test, vectorizer)\n",
        "\n",
        "# Define DataLoader to handle batching and shuffling\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "lueQYpzpdov_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.softmax(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model = NeuralNet(vocab_size,12,6)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "kf2svGifih09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            labels = labels.to(device) #.float()  # Convert labels to appropriate type\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')"
      ],
      "metadata": {
        "id": "pXVFuSsTljel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalModel(model,testloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Move model to the same device as the input data\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)  # Move input data to the same device as the model\n",
        "            labels = labels.to(device)  # Move labels to the same device as the model\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy on the test set: {correct / total:.3f}')"
      ],
      "metadata": {
        "id": "xzH6eR0-l6bC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()  # Cross Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "epochs=10"
      ],
      "metadata": {
        "id": "-5V3Tp-4mP0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, criterion, optimizer, train_loader, epochs)\n",
        "evalModel(model,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvlvTbkYmoaG",
        "outputId": "c4bb4415-c96b-41fc-dfd2-db1cb73a65cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.8516002047567421\n",
            "Epoch 2/10, Loss: 0.2560795852697462\n",
            "Epoch 3/10, Loss: 0.14165190855893484\n",
            "Epoch 4/10, Loss: 0.10042342752433502\n",
            "Epoch 5/10, Loss: 0.07808699159521022\n",
            "Epoch 6/10, Loss: 0.06531359147274379\n",
            "Epoch 7/10, Loss: 0.05918412831256501\n",
            "Epoch 8/10, Loss: 0.05411385391755958\n",
            "Epoch 9/10, Loss: 0.044986995890626345\n",
            "Epoch 10/10, Loss: 0.04355707082239727\n",
            "Accuracy on the test set: 0.842\n"
          ]
        }
      ]
    }
  ]
}